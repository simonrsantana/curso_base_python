{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje Supervisado con python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ¿Qué es el aprendizaje automático?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje automático, o _machine learning_, es una aplicación de Inteligencia Artificial (AI) que proporciona a un sistema la habilidad para aprender y mejorar a partir de unos datos sin la necesidad de que esto esté necesariamente incluido en la programación. El ML  is an application of artise centra principalmente en el desarrollo de programas computacionales que puedan leer y usar datos por sí solos.\n",
    "\n",
    "Ejemplos:\n",
    "- Sistema que predice si un email va a ser spam o no basándose en una serie de características del mismo\n",
    "- Sistema de recomendación de artículos a un usuario adaptado a sus gustos\n",
    "- Estimación del precio de una vivienda basándose en características de la misma\n",
    "- Agrupación por semejanza de los artículos de la wikipedia\n",
    "- Detección de comportamientos anómalos en una serie temporal\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Supervisado vs. no supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El _aprendizaje no supervisado_ trata de descubrir la estructura presente en un conjunto de datos: es decir, trata de buscar semejanzas (o diferencias) entre las observaciones.\n",
    "\n",
    "Ejemplo: segmentar una base de datos de clientes en grupos por semejanza entre éstos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El _aprendizaje supervisado_ utiliza _datos etiquetados_, y su objetivo es asignar una etiqueta a las observaciones futuras basándose en las características de las observaciones previas.\n",
    "\n",
    "Ejemplo: detectar si una transacción realizada con una tarjeta de crédito es fraudulenta o no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Regresión vs. clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dentro del aprendizaje supervisado, podemos distinguir a su vez dos tipos de problemas según el tipo de variable que estemos prediciendo:\n",
    "- Clasificación: la variable a predecir es categórica\n",
    "- Regresión: la variable a predecir es numérica\n",
    "\n",
    "__Ejemplo:__ ¿Cuál de los siguientes es un problema de clasificación? ¿Y de regresión?\n",
    "- Señalar si un texto tiene connotación positiva o negativa\n",
    "- Sistema que predice si un email va a ser spam o no basándose en una serie de características del mismo\n",
    "- Estimación del precio de una vivienda basándose en carcaterísticas de la misma\n",
    "- Estimación de la esperanza de vida de un sujeto\n",
    "- Detección de transacciones fraudulentas realizadas con una tarjeta de crédito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fundamentos del aprendizaje automático. Modelos lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a importar las librerías que necesitamos para arrancar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E importemos una base de datos sencilla y conocida para los primeros ejemplos. Esto nos ahorrará realizar el análisis exploratorio (ya lo hicimos parcialmente cuando vimos Pandas) e ir directamente al grano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas = pd.read_csv('data/houseprices.csv')\n",
    "\n",
    "print(casas.head())\n",
    "print(casas.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no convertiremos las variables categóricas a categorías puesto que scikit-learn, la librería de aprendizaje automático que usaremos, __no las maneja__ En todo caso, no las vamos a necesitar hasta dentro de un rato.\n",
    "\n",
    "## 2.1 La librería sklearn\n",
    "\n",
    "Importemos los módulos que necesitamos para entrenar un modelo de regresión. La librería sklearn es tan enorme que lo mejor es importar solo lo necesario para no saturar el entorno\n",
    "\n",
    "En nuestro caso emplearemos mayormente `linear_model`, un modelo lineal bastante simple para predicción (ajuste de rectas). Hay modelos muchísimo más sofisticados, algunos especialmente eficientes o con aplicaciones específicas, pero describirlos queda algo fuera de nuestro alcance ahora mismo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La API de sklearn hace que todos sus modelos (este en concreto es el modelo de regresión lineal) se comporten del mismo lo cual es muy conveniente para aprender a usar la librería, ya que aprendiendo a emplear uno de ellos podremos extrapolar al resto de modelos. Por tanto, si aprendemos a emplear este, no tendremos grandes problemas usando otras cosas como _random forests_, _support vector machines_ o las archiconocidas _redes neuronales_. \n",
    "\n",
    "Lo primero que tenemos que definir son los datos que usaremos como variable respuesta y los que usaremos como variables predictoras. \n",
    "\n",
    "> Para ambos casos __hay que pasar bien arrays de numpy, bien a dataframes de pandas con columnas numéricas__ \n",
    "\n",
    "> `sklearn` solamente maneja variables numericas\n",
    "\n",
    "\n",
    "En este caso tratamos de predecir `SalePrice` (el precio de venta) usando la variable `GrLivArea` (el área de la casa en pies al cuadrado). Solamente emplearemos una de las muchas variables a nuestra disposición, aunque podríamos hacer modelos más complejos (y mejores) que tengan en cuenta el resto. \n",
    "\n",
    "Para saber a qué corresponde cada variable de este dataset, consultar en https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definimos los arrays de predictores (X) y respuesta (y)\n",
    "y = casas[['SalePrice']]\n",
    "X = casas[['GrLivArea']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos de scikitlearn son clases. No hemos visto las clases en detalle, pero están ampliamente extendidas y además son muy fáciles de usar. Una clase nos permite crear objetos de un nuevo tipo, y podremos definir nuevas instancias dentro de ese tipo (las cuales pueden tener métodos y atributos propios). Ver las clases en detalle no entra en nuestros objetivos, pero es un aspecto importante que cubrir si quieren continuar su desarrollo en python. \n",
    "\n",
    "Para lo que nos es relevante, en este caso, crearemos los modelos invocando al constructor de la clase (`linear_model.LinearRegression()` en este caso): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inicializamos el modelo (creamos una instancia del modelo para emplearlo)\n",
    "reg_casas = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creado, lo iremos modificando e inspeccionando a través de diferentes métodos.\n",
    "\n",
    "Los modelos se __entrenan__ (se ajustan) con el método `fit(predictores, respuesta)`, que incorpora los datos al modelo. Recordemos que, en general, los modelos de sklearn se tratan **todos igual**, y por tanto con esto tendremos un ejemplo simple que seguir si queremos emplear cualquier otro modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lo entrenamos con los datos\n",
    "reg_casas.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué hemos obtenido con esto? Investiguemos un poco los métodos que tiene el objeto que acabamos de entrenar.\n",
    "\n",
    "Podemos obtener los coeficientes del modelo, por ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_casas.coef_ # Coeficiente de la regresión lineal (pendiente de la recta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para predecir el valor de  nuevas observaciones, usamos el método `.predict()`. Como en este caso no tenemos nuevas observaciones, podemos predecir las que tenemos, que recordemos eran casas['SalePrice'] (¡OJO! ¡Esta práctica de testear el modelo en los datos de train no suele ser una buena idea..!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lo usamos para predecir\n",
    "print(reg_casas.predict(X))\n",
    "print(np.array(casas['SalePrice']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ¿Qué tal predice mi modelo?\n",
    "\n",
    "Para hacernos una idea de la precisión de las predicciones, necesitamos alguna función. En regresión se suele usar el _error cuadrático medio_, es decir, la suma de cuadrados de los residuos (la diferencia entre la predicción y la realidad).\n",
    "\n",
    "O, de una manera equivalente, el $R^2$, que normaliza esta suma de cuadrados con la varianza de los datos y que podemos interpretar como la proporción de la varianza explicada por el modelo.\n",
    "\n",
    "En sklearn lo podemos calcular con el método `score(predictores, respuesta)` Lo que hace entonces sklearn es comerse la array de predictores, calcular el resultado de aplicarles el modelo, y comparar este resultado con la verdadera respuesta para calcular el $R^2$.\n",
    "\n",
    "El $R^2$ es una métrica de cuánta varianza de los datos estamos explicando con nuestro modelo. Para un modelo linearl, el $R^2$ queda definido como (1 - [varianza residual / varianza de la variable dependiente]). Por tanto, si un modelo explica perfectamente la varianza de los datos, tendrá un $R^2 = 1$\n",
    "\n",
    "En este caso, al estar prediciendo los datos que hemos usado para entrenar, estamos calculando el error residual del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtenemos la estimación del R2\n",
    "reg_casas.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ejercicio: ¡Un primer modelado!\n",
    "\n",
    "- Leer la base de datos 'data/wages.csv'\n",
    "- Hacer un rápido análisis exploratorio\n",
    "- Crear un modelo lineal llamado reg_wage \n",
    "- Entrenarlo usando como predictor la columna age y como respuesta earn\n",
    "- Predecir usando las columnas del dataframe y comparar con las realmente observadas\n",
    "- Calcular el $R^2$ del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rellena el código que falta\n",
    "'''\n",
    "\n",
    "## Leer la base de datos 'data/wages.csv' y mostrar el encabezado del dataframe\n",
    "\n",
    "wages = pd.read_csv(____)\n",
    "\n",
    "print(wages.head())\n",
    "\n",
    "## Definimos la variable respuesta (columna earn) y la predictora (columna age)\n",
    "\n",
    "y = ___\n",
    "X = ___\n",
    "\n",
    "## Definir el modelo como hemos hecho más arriba pero usando X_2, y_2 y llamadlo reg_wage\n",
    "\n",
    "reg_wage = ___\n",
    "\n",
    "## Entrenar el modelo con .fit(X, y)\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "print('-------\\n Predicciones:')\n",
    "print(reg_wage.predict(X))\n",
    "print(np.array(casas['SalePrice']))\n",
    "\n",
    "print('-------\\n R2:')\n",
    "print(reg_wage.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Conjuntos de entrenamiento y test (S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar el overfitting tenemos que evaluar el modelo en una serie de datos que no hayamos usado en ningún momento del proceso de ajuste del modelo. \n",
    "\n",
    "> Hay que ser muy cuidadosos y aislar totalmente los datos con los que se construye el modelo de los datos con los que vamos a estimar su error\n",
    "\n",
    "Para hacernos una idea aproximada de cómo va a funcionar cuando le presentemos nuevos datos, tenemos que guardarnos una parte de nuestra base de datos, que __no usaremos para entrenar el modelo__. Sólo la usaremos para predecir, y con lo que obtengamos de ella, calcularemos las métricas de interés.\n",
    "\n",
    "El subconjunto de datos que vamos a usar para entrenar se llama __conjunto de entrenamiento__ (train set), mientras que el que usamos para evaluarlo se llama __conjunto de test__ (test set)\n",
    "\n",
    "Aunque podríamos separar los datos nosotros mismos si nos conveniera, podemos hacerlo también con sklearn, puesto que tiene una función integrada preparada para esto precisamente. La función que tenemos que importar es `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = casas[['GrLivArea']], casas[['SalePrice']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miremos a ver qué dimensiones tienen los subconjuntos que hemos creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X train {}'.format(X_train.shape))\n",
    "print('Y train {}'.format(y_train.shape))\n",
    "print('X test {}'.format(X_test.shape))\n",
    "print('Y test {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repitamos ahora el proceso de entrenamiento y predicción, pero esta vez usando el conjunto de train para entrenar y el de test para calcular el $R^2$.\n",
    "\n",
    "Vamos a calcular el $R^2$ que obtenemos en el conjunto de test y en el train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instanciamos el modelo y lo entrenamos con los datos de entrenamiento\n",
    "reg_casas = linear_model.LinearRegression()\n",
    "reg_casas.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('-------\\n R2 test:')\n",
    "\n",
    "print(reg_casas.score(X_test, y_test))\n",
    "\n",
    "print('-------\\n R2 train:')\n",
    "\n",
    "print(reg_casas.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Obviamente el R2 del conjunto de test es más bajo, pero más cercano a la _realidad_\n",
    "\n",
    "> Que no haya demasiada diferencia entre ambos es buena señal: no parece que haya **sobreajuste** (overfitting). Pero claro, nos interesará mejorar ese $R^2$, ¡¡y el único modo es usando modelos más complejos!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overfitting](figures/underfitting_and_overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: Predicción básica de salarios \n",
    "Estimar usando una partición en train/test el $R^2$ del modelo de los salarios. \n",
    "Mostrar el $R^2$ que obtenemos en el conjunto de test y en el train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rellena el código que falta\n",
    "'''\n",
    "## Leemos la base de datos '../data/wages.csv'\n",
    "wages = pd.read_csv(____)\n",
    "\n",
    "## Definimos la variable respuesta (columna earn) y la predictora (columna age)\n",
    "\n",
    "y = ___\n",
    "X = ___\n",
    "\n",
    "\n",
    "### Crear particion en train/test con train_test_split()\n",
    "\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "\n",
    "### Definimos y entrenamos el modelo\n",
    "reg_wages = ___\n",
    "reg_wages.fit(X_train, y_train)\n",
    "\n",
    "### Visualicemos el R2 del modelo usando los conjuntos de train y test\n",
    "\n",
    "print('-------\\n R2 test:')\n",
    "\n",
    "print(reg_wages.score(X_test, y_test))\n",
    "\n",
    "print('-------\\n R2 train:')\n",
    "\n",
    "print(reg_wages.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Validación cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviamente, realizar esta operación una sóla vez puede darnos un resultado demasiado azaroso, sea esto para bien o para mal. Por ello deberíamos repetir este proceso varias veces y calcular su media, para así tener una mejor estimación de lo que podemos esperar cuando lo probemos con datos reales y no depender de la suerte.\n",
    "\n",
    "Este proceso se conoce como __validación cruzada__ y está implementado en sklearn. El núcleo de esta idea no es más que tomar los datos y dividirlos en train/test un cierto número de veces, haciendo que cada vez el conjunto de train y de test no sea igual al anterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función que usaremos será `cross_val_score(modelo, datos, número de pliegues)`\n",
    "\n",
    "El número de \"pliegues\" o \"cortes\" que hagamos afecta el valor que obtenemos. Un valor en torno a 10 es razonable para la mayor parte de modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![validación cruzada](figures/cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La función nos devuelve una array con los valores del R2 de cada proceso de entrenamiento/test. Por lo tanto, no hay que dividir previamente en el conjunto de test. Ya lo hace la función `cross_val_score` por nosotros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas = pd.read_csv('data/houseprices.csv')\n",
    "y = casas[['SalePrice']]\n",
    "X = casas[['GrLivArea']]\n",
    "reg_casas = linear_model.LinearRegression()\n",
    "\n",
    "\n",
    "cv_results = cross_val_score(reg_casas, X, y, cv=10)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `cross_val_score` es un _wrapper_ sobre la función `.fit`, es decir, una función que tiene anidada otra función, en este caso el `.fit()`, a la cual va llamando después de realizar las sucesivas divisiones en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_results.mean())\n",
    "print(cv_results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ejercicio: Validación cruzada de las predicciones en salarios\n",
    "\n",
    "Estimar por validación cruzada el $R^2$ del modelo que explica el salario con el nivel educativo del trabajador. Mostrar los resultados de la validación, su media y su desviación típica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rellena el código que falta\n",
    "'''\n",
    "\n",
    "## Leemos la base de datos 'data/wages.csv'\n",
    "wages = pd.read_csv(____)\n",
    "\n",
    "## Definimos la variable respuesta (columna earn) y la predictora (columna age)\n",
    "\n",
    "y = ___\n",
    "X = ___\n",
    "\n",
    "\n",
    "### Crear particion en train/test con train_test_split()\n",
    "\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "\n",
    "### Definimos el modelo\n",
    "\n",
    "reg_wages = ___\n",
    "\n",
    "### Estimar el R2 por validación cruzada en 10 pliegues\n",
    "cv_results = ___\n",
    "print(cv_results)\n",
    "\n",
    "print(cv_results.mean())\n",
    "print(cv_results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Observamos cómo, en este caso, al haber realizado antes sólo una validación, los resultados del conjunto de test eran malos _pero por mala suerte_. La estimación realizada por validación cruzada es bastante más fiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Más predictores\n",
    "\n",
    "Podemos incorporar más predictores al modelo pasándolos _como una lista_\n",
    "\n",
    "`sklearn` __únicamente maneja números__ así que por el momento no vamos a poder usar predictores categóricos en nuestros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vamos e entrenar el modelo con todas las columnas numéricas\n",
    "\n",
    "## Leemos los datos\n",
    "casas = pd.read_csv('data/houseprices.csv')\n",
    "\n",
    "## Definimos los predictores\n",
    "predictores = list(casas.select_dtypes(['number']).columns)\n",
    "predictores.remove('SalePrice')\n",
    "\n",
    "## Definimos las arrays de predictores y de respuesta\n",
    "y = casas[['SalePrice']]\n",
    "X = casas[predictores]\n",
    "\n",
    "## Definimos el modelo, lo entrenamos y lo evaluamos por cv\n",
    "reg = linear_model.LinearRegression()\n",
    "cv_results = cross_val_score(reg, X, y, cv=10)\n",
    "\n",
    "print(cv_results)\n",
    "print(np.mean(cv_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podría parecer que mejora el resultado (que lo hace) pero mucho ojo con esto porque\n",
    "\n",
    "> __Usar el conjunto de test para evaluar los diferentes modelos conduce al overfitting__ \n",
    "\n",
    "Para seleccionar un modelo de entre varios, hay que usar otra partición de los datos (**el conjunto de validación**), de forma que no estemos empleando en el entrenamiento información que debe estar reservada para el testing. No vamos a ver este concepto en profundidad, pero es crucial para hacer comparaciones justas entre modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: Predicciones más complejas de los salarios  \n",
    "\n",
    "Entrenar un modelo lineal con todos los predictores numéricos de wages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rellena el código que falta\n",
    "'''\n",
    "\n",
    "### Vamos e entrenar el modelo con todas las columnas numéricas\n",
    "\n",
    "## Leemos la base de datos 'data/wages.csv'\n",
    "wages = pd.read_csv('data/wages.csv')\n",
    "\n",
    "## Definimos la lista de predictores (todas las columnas numericas )\n",
    "predictores = list(wages.select_dtypes(['number']).columns)\n",
    "predictores.remove('earn')\n",
    "\n",
    "## Definimos la variable respuesta (columna earn) y la predictora (columna age)\n",
    "\n",
    "y = wages[['earn']]\n",
    "X = wages[predictores]\n",
    "\n",
    "\n",
    "## Definimos el modelo,  y lo evaluamos por cv\n",
    "reg = linear\n",
    "cv_results = ___\n",
    "\n",
    "print(cv_results)\n",
    "print(np.mean(cv_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principales métodos de sklearn para entrenamiento de los modelos\n",
    "\n",
    "- linear_model.LinearRegression() (o importar el método en el que estemos interesados)\n",
    "- modelo.fit(X, y)\n",
    "- modelo.predict(X)\n",
    "- modelo.score(X, y)\n",
    "- model_selection.train_test_split\n",
    "- model_selection.cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocesado de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de usar un algoritmo de aprendizaje automático, suele ser imprescindible preprocesar los datos para:\n",
    "\n",
    "- Imputar valores ausentes (NAs)\n",
    "- Normalización de los datos\n",
    "- Eliminar atípicos o errores\n",
    "- Arreglar varianzas desproporcionadas, puntos influyentes o palanca\n",
    "- Reducir de la dimensión\n",
    "- Ingeniería de predictores\n",
    "\n",
    "> IMPORTANTE: Todos estas operaciones de preprocesado hay que realizarlas ÚNICAMENTE considerando el conjunto de entrenamiento, y hacerlas EXACTAMANENTE IGUAL con el conjunto de test. \n",
    "\n",
    "De no hacerlo así, estaríamos filtrando información del conjunto de test lo cual _falsearía_ la estimación del error\n",
    "\n",
    "> ¡¡¡El __overfitting__ está al acecho en todos los pasos del proceso de entrenamiento de un modelo, no sólo en el momento de estimar el error!!!\n",
    "\n",
    "> - Durante el preprocesado\n",
    "> - Durante la selección de parámetros\n",
    "> - Durante la selección del modelo\n",
    "> - En la estimación del error\n",
    "\n",
    "\n",
    "## 3.1 Dos observaciones MUY importantes\n",
    "\n",
    "Aunque `sklearn` admite dataframes como entradas en los modelos y para todas las operaciones de preprocesado como pronto veremos, internamente trabaja __con arrays de numpy__ __y cualquier operación que realice devolverá así mismo una array.__\n",
    "\n",
    "Esto es fundamental para evitar los errores o para depurar el código. Es un poco incómodo por ser de nivel más bajo que, por ejemplo, `statsmodels`, pero eso se compensa con la flexibilidad que proporciona.\n",
    "\n",
    "Por otra parte las clases de operaciones de preprocesado de `sklearn` funcionan exactamente como los modelos, algo muy útil como veremos más adelante:\n",
    "\n",
    "- Primero lo ajustamos a los datos de entrenamiento (con el método `fit`) \n",
    "- Y luego usaremos ese objeto  para transformar la columna correspondiente (`transform`), sea la del conjunto de test o la de train\n",
    "\n",
    "> En una problema real, tendríamos SIEMPRE que ajustar la operación de preprocesado con el conjunto de train y a continuación transformar el conjunto de train y el de test con los parámetros así obtenidos. En caso contrario, estaríamos usando __información del conjunto de test para entrenar el modelo__ y esto produciría overfitting\n",
    "\n",
    "\n",
    "## 3.2 Imputación de NAs\n",
    "\n",
    "Los valores ausentes son observaciones a las que les falta el valor de alguna de las variables. Normalmente, los algoritmos de aprendizaje no los pueden procesar y, o fallan o los omiten.\n",
    "\n",
    "La opción de omitir los NAs siempre está ahí, pero muchas veces suele rentar tratar de _IMPUTAR_ esos valores, es decir, sustituirlos por una estimación razonable del valor que podría tener. Para realizar esto, hay varios métodos:\n",
    "\n",
    "- Sustituirlos por el valor más común de la distribución (mediana o media para las distribuciones continuas, moda para las discretas)\n",
    "- Crear una nueva categoría (sólo válido para variables categóricas)\n",
    "- Entrenar un algoritmo de aprendizaje automático que prediga los valores ausentes\n",
    "\n",
    "> Hagamos lo que hagamos, es fundamental que las decisiones se tomen sólo considerando el conjunto de entrenamiento, y que la imputación se realice del mismo modo en el conjunto de test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "casas = pd.read_csv('data/houseprices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examinemos primero si tenemos NAs en el dataframe\n",
    "\n",
    "nulos = casas.isnull()\n",
    "suma_nulos = nulos.sum()\n",
    "ordenados = suma_nulos.sort_values(ascending=False)\n",
    "\n",
    "print(ordenados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vemos qué significan las distintas columnas del dataset veremos que solamente los valores nulos de Electrical son realmente valores nulos (para `GarageType` y `BsmtQual` simplemente simbolizan que la casa en cuestión no tiene garaje o sótano). Así pués, vamos a arreglar estas columnas con valores nulos \"falsos\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas.GarageType = casas.GarageType.fillna(\"NoGarage\")\n",
    "casas.BsmtQual = casas.BsmtQual.fillna(\"NoBsmt\")\n",
    "nulos = casas.isnull()\n",
    "suma_nulos = nulos.sum()\n",
    "ordenados = suma_nulos.sort_values(ascending=False)\n",
    "print(ordenados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la variable Electrical, los NAs son verdaderamente NAs. Para arreglarlos podemos:\n",
    "\n",
    "- Eliminar las observaciones\n",
    "- Imputar un valor. Generalmente, la moda\n",
    "- Crear una nueva categoría\n",
    "- Hacer un algoritmo de aprendizaje que use el resto de columnas para conjeturar el posible valor (más sobre esto en la segunda parte del curso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminar las observaciones a missing, basta con usar el método `dropna`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas_sinNA = casas.dropna()\n",
    "len(casas), len(casas_sinNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar las observaciones con NAs puede ser una estrategia razonable cuando se trata de un número pequeño de observaciones, pero cuando hay una gran proporción, es más interesante _imputarlas_. Hay varias maneras de imputar, pero hoy vamos a ver sólo las más sencillas: imputación de la mediana para las variables numéricas y de la moda para las categóricas\n",
    "\n",
    "Las clases de imputadores de sklearn (`Imputer`)  funcionan exactamente como los modelos, algo muy útil como veremos más adelante:\n",
    "\n",
    "- Primero lo ajustamos a los datos de entrenamiento (con el método `fit`) \n",
    "- Y luego lo podemos usar para transformar la columna (`transform`), sea la del conjunto de test o la de train\n",
    "\n",
    "Por ejemplo, imputemos valores a una columna numérica, y transformémosla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vamos a crear un Nan en SalePrice, que no tiene ninguno\n",
    "casas.loc[1220, 'SalePrice'] = np.nan\n",
    "print(casas['SalePrice'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importamos el imputador y lo creamos\n",
    "# strategy puede ser mean, median o most_frequent\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median') # , axis=0) # En versiones anteriores necesita el eje\n",
    "\n",
    "# Lo entrenamos\n",
    "imp.fit(casas[['SalePrice']])\n",
    "\n",
    "# Esta línea transforma la columna casas[['SalePrice']] \n",
    "casas[['SalePrice']] = imp.transform(casas[['SalePrice']])\n",
    "\n",
    "##  Comprobemos que ha imputado\n",
    "print(casas['SalePrice'].isnull().sum())\n",
    "\n",
    "## Y qué ha imputado\n",
    "print(casas.loc[1220, 'SalePrice'])\n",
    "print(casas.SalePrice.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por cierto: la operación de ir preprocesando los datos de una manera secuencial es ir modificando el dataframe o almacenarlo en uno nuevo __es muy poco recomendable__: acabaremos con el entorno lleno de objetos y las posibilidades de errores se disparan.\n",
    "\n",
    "Afortunadamente luego veremos cóm encapsular estas operaciones en una _tubería_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para el caso de las categóricas podemos emplear la misma función, aunque hay que prestar atención al método que se selecciona. En este caso tendremos solamente disponibles `most_frequent` y `constant`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Miramos las distribución de la variable primero\n",
    "casas.Electrical.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creamos el imputador como antes\n",
    "cat_imp = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "## Creamos una copia del dataset para hacer pruebas sin alterar el original\n",
    "casas_imp = casas.copy()\n",
    "\n",
    "## Ajustamos el imputador\n",
    "cat_imp.fit(casas_imp[['Electrical']])\n",
    "\n",
    "## Hacemos la imputación, sobreescribiendo el dataframe\n",
    "casas_imp[['Electrical']] = cat_imp.transform(casas_imp[['Electrical']])\n",
    "\n",
    "## ¿Habrá cambiado la distribución de las variables?\n",
    "print(casas_imp.Electrical.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Insistimos: lo de alterar el dataframe en cada operación es sólo una conveniencia pedagógica. Trabajar así en la práctica está abocado a la catástrofe!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: Imputación de NAs para salarios\n",
    "\n",
    "Carga la tabla `wages.csv`. Explora la tabla para saber cuántos NAs hay y trata de imputarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rellena el código que falta\n",
    "'''\n",
    "\n",
    "wages = pd.read_csv('data/wages.csv')\n",
    "\n",
    "## Examinemos primero si tenemos NAs en el dataframe\n",
    "\n",
    "nulos = ___\n",
    "suma_nulos = ___\n",
    "ordenados = ___\n",
    "\n",
    "print(ordenados)\n",
    "\n",
    "## Visualicemos la distribución de valores de la columna race con value_counts\n",
    "\n",
    "print(___)\n",
    "\n",
    "## Definimos el imputador categórico\n",
    "\n",
    "cat_imp = ___\n",
    "\n",
    "## Lo usamos para transformar la columna race\n",
    "\n",
    "wages.race = ___\n",
    "\n",
    "## Visualicemos la distribución de valores de la columna race con value_counts\n",
    "\n",
    "print(___)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.3 Estandarización de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estandarización de nuestro dataset es un requisito preliminar para muchos algoritmos de ML: éstos podrían comportarse mal si las features individuales no parecen más o menos datos distribuidos como una normal estándar ($\\mathcal{N}(0,1)$). En la práctica, frecuentemente olvidaremos la forma de la distribución y simplemente transformaremos los datos centrándolos (quitando la media) y dividiendo entre su desviación estándar. \n",
    "\n",
    "Por ejemplo, en algunas funciones objetivo el hecho de que alguna columna tenga mucha más varianza que otras puede engañar al algoritmo, de forma que sea incapaz de prestar atención del resto de columnas.\n",
    "\n",
    "Manos a la obra. Como no tiene sentido calcular media ni std para columnas categóricas, en esta sección solo trabajaremos con las columnas numéricas (más adelante veremos cómo representar las categóricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas_num = casas._get_numeric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler().fit(casas_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas_num_sc = sc.transform(casas_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: al hacerlo así, podremos volver a usar `sc.transform` sobre el conjunto de test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "casas_num.loc[:,:] = casas_num_sc\n",
    "casas_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "casas_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas_num_sc = casas_num.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: Renormalización en las variables numéricas de salarios\n",
    "\n",
    "Aplica escalados a las variables numéricas de `wages.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rellena el código que falta\n",
    "'''\n",
    "## Definimos las columnas numericas\n",
    "\n",
    "wages_num = ___\n",
    "\n",
    "wages_num.describe()\n",
    "\n",
    "## Definamos el normalizador\n",
    "\n",
    "sc_wages = __\n",
    "\n",
    "## Usémoslo para transformar todo el dataframe de columnas numéricas\n",
    "\n",
    "wages_num = ___\n",
    "\n",
    "wages_num.shape\n",
    "\n",
    "print(np.mean(wages_num[:,1]))\n",
    "print(np.std(wages_num[:,1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Variables categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hace algunas secciones atrás hemos visto una forma de representar variables categóricas, asignándolas un número natural a cada valor diferente mediante `LabelEncoder`. Sin embargo, esto no es suficiente ya que los estimadores de sklearn esperan inputs continuos/ordenados.\n",
    "\n",
    "Por tanto, la representación estándar para variables categóricas es la de one-hot-encoding, fácil de ver con un ejemplo. Supongamos que tenemos una variable que toma tres valores `Rojo`, `Azul`  y `Verde`. Basta pues con crear tres nuevas variables numéricas (las llamadas dummy variabels) y hacer la siguiente transformación (recordad cuando teníamos variables categóricas en la clase de regresión lineal):\n",
    "\n",
    "* `Rojo` -> (1, 0, 0).\n",
    "* `Azul` -> (0, 1, 0).\n",
    "* `Verde` -> (0, 0, 1).\n",
    "\n",
    "Mediante pandas, basta usar `get_dummies`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas_dum = pd.get_dummies(casas)\n",
    "casas_dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas_dum.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera, calculamos las dummies de wages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wages = pd.read_csv(\"data/wages.csv\")\n",
    "pd.get_dummies(wages).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de usar pandas, también es posible crear las variables dummies directamente a través de sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn_pandas import gen_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos en dos listas los nombres de las variables continuas y en otra las categóricas, puesto que van a ser tratadas de diferente modo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictores_cat = list(casas_imp.select_dtypes(['object']).columns)\n",
    "predictores_num = list(casas_imp.select_dtypes(['number']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos por medio de las funciones `gen_features` y `DataFrameMapper` las operaciones a realizar sobre esas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import gen_features, DataFrameMapper\n",
    "\n",
    "cat_factory = gen_features(\n",
    "     columns=predictores_cat,\n",
    "     classes=[LabelBinarizer]\n",
    " )\n",
    "num_factory = gen_features(\n",
    "    columns=predictores_num,\n",
    "    classes=[None]\n",
    ")\n",
    "factory = DataFrameMapper(\n",
    "    cat_factory + num_factory, df_out=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory.fit_transform(casas_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recapitulando...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto como las fases típicas del preprocesado de datos:\n",
    "1. Imputación de NAs\n",
    "2. Estandarización\n",
    "3. Filtrado de NAs\n",
    "4. Codificación de variables categóricas\n",
    "\n",
    "Estos pasos se pueden realizar de 1 en 1 \"manualmente\", pero si se encapsulan de forma que sean compatibles con sklearn, será posible volver a repetir las mismas operaciones sobre nuevos datos. Por ejemplo, volvemos a aplicar el preprocesado a las 100 primeras observaciones de casas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas = pd.read_csv('data/houseprices.csv')\n",
    "casas_new = casas[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas_new_imp = casas_new.copy()\n",
    "\n",
    "## Hacemos la imputación como antes\n",
    "cat_imp.fit(casas_new[['Electrical']])\n",
    "casas_new_imp[['Electrical']] = cat_imp.transform(casas_new[['Electrical']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas_new_sc = casas_new_imp._get_numeric_data().copy()\n",
    "casas_new_sc.iloc[:,:] = sc.transform(casas_new_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principales métodos de sklearn para preprocesado de los datos\n",
    "\n",
    "- from sklearn.preprocessing import Imputer\n",
    "- from sklearn_pandas import CategoricalImputer\n",
    "- from sklearn.preprocessing import StandardScaler\n",
    "- from sklearn.preprocessing import LabelBinarizer\n",
    "- from sklearn_pandas import gen_features\n",
    "- from sklearn_pandas import DataFrameMapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sistematizado el proceso: cañerías (OPCIONAL)\n",
    "\n",
    "No hemos hecho más que empezar, y ya estamos viendo el nivel de complejidad que puede alcanzar un problema de machine learning, la cantidad de transformaciones que hay que aplicar, transformaciones que hay que cuidarse de hacer sólo en el conjunto de entrenamiento...\n",
    "\n",
    "Por ejemplo, un proceso típico sería\n",
    "\n",
    "- Limpieza\n",
    "- Imputación\n",
    "- Escalado\n",
    "- Creación de dummies\n",
    "- Selección y entrenamiento del modelo\n",
    "- Optimización  del modelo\n",
    "- Calcular su error\n",
    "- __Y todo esto cuidando de dividir repetidas veces el conjunto en entrenamiento y test y de realizar las operaciones en ambos con los mismos parámetros__\n",
    "\n",
    "Las posibilidades de error o de que haya filtraciones entre uno y otro subconjunto son muy elevadas. Es por ello que se hace necesario sistematizar y blindar todo el proceso.\n",
    "\n",
    "\n",
    "> Afortunadamente, sklearn pone a nuestra disposición un procedimiento denominado tubería (pipeline) que nos simplifica la vida: sencillamente vamos definiendo las diferentes operaciones que vamos a hacer a los datos de manera simbólica y cómo se enganchan unas con otras, construyendo de esta manera una \"cañería\" por la que van a \"fluir\" los datos, que ejecutaremos como si fuera un sólo objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejemplo sencillo de pipeline\n",
    "\n",
    "## Importamos las funciones que vamos a usar en el modelo\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## Leemos los datos y definimos los predictores\n",
    "## En este caso, vamos simplemente a usar los predictores numéricos\n",
    "casas = pd.read_csv('data/houseprices.csv')\n",
    "predictores = list(casas.select_dtypes(['number']).columns)\n",
    "predictores.remove('SalePrice')\n",
    "\n",
    "## Definimos las arrays de predictores y de respuesta\n",
    "y = casas[['SalePrice']]\n",
    "X = casas[predictores]\n",
    "\n",
    "\n",
    "## Definimos la cañería\n",
    "steps = [('Reescalado', StandardScaler()),\\\n",
    "         ('Regresion', linear_model.LinearRegression())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "\n",
    "## Ajustamos la cañería a los datos como si fuera un modelo\n",
    "pipeline.fit(X,y)\n",
    "\n",
    "## También podemos estimar el error del modelo por validación cruzada\n",
    "scores = cross_val_score(pipeline, X, y, cv=10)\n",
    "\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Es así de sencillo! De esta forma tendremos mucho más controlado lo que le hacemos a los datos, asegurándonos de que lo hacemos de forma sistemática y consistente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Ejercicio: Cañerías aplicadas a los salarios \n",
    "\n",
    "Con los datos de Wages.csv sin tratar (¡leerlos de nuevo si hace falta!) ajustar una cañería que:\n",
    "\n",
    "- Prediga con un modelo lineal usando todos los predictores numéricos pero que hayan sido escalados previamente\n",
    "- Añadir un imputador strategy='most_frequent'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rellena el código que falta\n",
    "'''\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "## Leemos los datos y definimos los predictores\n",
    "## En este caso, vamos simplemente a usar los predictores numéricos\n",
    "wages = pd.read_csv('data/wages.csv')\n",
    "predictores = ___\n",
    "predictores.remove('earn')\n",
    "\n",
    "## Dividimos en entrenamiento y test\n",
    "y = ___\n",
    "X = ___\n",
    "\n",
    "\n",
    "## Definimos los pasos\n",
    "steps = [('Escalado',  ___),\\\n",
    "         ('Regresion', ___)]\n",
    "\n",
    "## Definimos la cañería\n",
    "pipeline = ___\n",
    "\n",
    "\n",
    "## Estimamos el error del modelo por validación cruzada\n",
    "scores = cross_val_score(___)\n",
    "\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.2 ¿Necesita un fontanero?\n",
    "\n",
    "Las cañerías también pueden _bifurcarse_ de modo que las operaciones sólo afecten a una parte de los datos. Para ello necesitamos las funciones de `sklearn` como `ColumnTransformer`, con el cual podremos operar independientemente en columnas de datos numéricos y categóricos. \n",
    "\n",
    "La gran ventaja será que podremos probar diferentes operaciones de preprocesado de una manera muy sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los datos y definimos los predictores, distinguiendo entre numéricos y categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas = pd.read_csv('data/houseprices.csv')\n",
    "\n",
    "predictores_num = casas.select_dtypes(include=['number']).columns\n",
    "predictores_num = predictores_num.delete(-1)               # Eliminamos \"SalePrice\", este será nuestro target\n",
    "predictores_cat = casas.select_dtypes(include=['object']).columns\n",
    "\n",
    "y = casas.SalePrice                                        # Datos de salida\n",
    "\n",
    "X = casas[list(predictores_num) + list(predictores_cat)]   # Construimos lo que serán nuestros datos de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por medio de las funciones convenientes de sklearn, a cada feature categórica, aplicaremos una imputación categórica (que consiste en rellenar NaNs con la moda de la columna), y un LabelBinarizer, que pasa a obtener una representación one-hot binaria (recordad la clase de regresión lineal de la parte de estadística)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que categorical_features no es más que una lista de tuplas, indicando la columna y las operaciones que se harán a cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictores_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las transformaciones que queremos hacer a ambos tipos de variables, tanto las categóricas como las numéricas. En este caso, haremos lo siguiente:\n",
    "* **Numéricas**: Imputaremos los valores NA con la media del resto de valores y escalaremos los valores (estandarizamos los datos)\n",
    "* **Categóricas**: Imputamos los NAs con la moda (el valor más frecuente) y pasamos estas variables a formato \"one-hot-encoding\" para tratarlas en el resto del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformaciones_numericas = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan, strategy='median') ),\n",
    "                                      ('scaler',StandardScaler())])\n",
    "transformaciones_categoricas = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el procesador que tratará las columnas con la función `ColumnTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "preprocesador = ColumnTransformer(transformers=[('num', transformaciones_numericas, predictores_num),\n",
    "                                               ('cat', transformaciones_categoricas, predictores_cat)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, añadimos un `SelectKBest` para que haga anova (un método de análisis de la varianza de los datos) y elija las 15 mejores variables, y finalmente una regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[('preprocesador', preprocesador),\n",
    "       ('anova_filter', SelectKBest(k=15)), \n",
    "       ('predictor', linear_model.LinearRegression())]\n",
    "\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, obtenemos el rendimiento del algoritmo contrastado a través de _cross validation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(pipeline, X, y, cv= 10)\n",
    "\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Tenemos una precisión final (en train) del 80%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos ajustar el modelo a nuestros datos de entrenamiento y posteriormente predecir los de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2_train, X_2_test, y_2_train, y_2_test = train_test_split(X, y, test_size=0.25, random_state=2)\n",
    "pipeline.fit(X_2_train, y_2_train)\n",
    "pipeline.score(X_2_train, y_2_train), pipeline.score(X_2_test, y_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡En test recibimos una precisión bastante buena también! (~73%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero en definitiva, estas tuberías nos permiten abstraernos y que todo siga su cauce\n",
    "\n",
    "## Ejercicio: Predicción de salarios usando todo a la vez\n",
    "\n",
    "Realizar una cañería con la base de datos wages que:\n",
    "\n",
    "- Impute y convierta a dummies las variables categóricas\n",
    "- Escale las variables numéricas\n",
    "- Seleccione los mejores 7 predictores con un filtro anova\n",
    "- Ajuste un modelo lineal a la variable 'earn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rellena el código que falta\n",
    "'''\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "## Ejercicio: Una cañería sencillita\n",
    "\n",
    "## Leemos los datos y definimos los predictores\n",
    "## En este caso, vamos simplemente a usar los predictores numéricos\n",
    "\n",
    "wages = pd.read_csv('data/wages.csv')\n",
    "predictores_cat = ___\n",
    "predictores_num = ___\n",
    "\n",
    "# Elimina de predictores_num la columna 'earn', que será nuestro objetivo ahora\n",
    "\n",
    "## Dividimos en entrenamiento y test\n",
    "y = ___\n",
    "X = ___\n",
    "\n",
    "\n",
    "### Operaciones a realizar a las variables categóricas\n",
    "cat_factory = gen_features(\n",
    "     columns = ___,\n",
    "     classes = [___, ___]\n",
    " )\n",
    "\n",
    "### Concatenarlas con las operaciones a realizar a las variables numéricas\n",
    "factory = DataFrameMapper(\n",
    "    cat_factory + \n",
    "    [(___, ___)] \n",
    ")\n",
    "\n",
    "\n",
    "## Definimos la cañería\n",
    "steps = [('feat_prepro', ___), \n",
    "         ('anova_filter', ___), \n",
    "         ('predictor', ___) ]\n",
    "\n",
    "pipeline = ___\n",
    "\n",
    "## Estimamos el error del modelo por validación cruzada\n",
    "score = cross_val_score(pipeline, X, y, cv= 10)\n",
    "\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principales métodos de sklearn para control de flujo\n",
    "\n",
    "- from sklearn.pipeline import Pipeline\n",
    "- from sklearn_pandas import gen_features\n",
    "- from sklearn_pandas import DataFrameMapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importemos en un dataframe titanic el archivo titanic.csv y hagamos un análisis exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leemos los datos\n",
    "titanic = pd.read_csv('data/titanic_data.csv')\n",
    "\n",
    "### Exploramos\n",
    "print(titanic.head())\n",
    "print(titanic.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Observamos lo siguiente:\n",
    "\n",
    "- Hay 3 columnas que no vamos a usar por tratarse de valores únicos (Name, Ticket, Cabin)\n",
    "- Descontando esas, nos quedan dos categóricas (Embarked y Sex)\n",
    "\n",
    "\n",
    "Eliminemos las columnas que no vayamos a usar: Name, Ticket, Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for columna in ['Name', 'Ticket', 'Cabin']:\n",
    "    del titanic[columna]\n",
    "    \n",
    "titanic.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Vamos a ver los NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos = titanic.isnull()\n",
    "suma_nulos = nulos.sum()\n",
    "ordenados = suma_nulos.sort_values(ascending=False)\n",
    "\n",
    "print(ordenados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sólo hay nulos en columnas numéricas, así que podremos usar sin mayor problema el imputador de sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos a definir las operaciones que le vamos a aplicar, que serán:\n",
    "\n",
    "- Columnas numéricas:\n",
    "    - Imputar NAs\n",
    "    - Estandarizar\n",
    "- Columnas categóricas:\n",
    "    - Convertir a dummies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn_pandas import gen_features, DataFrameMapper\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "### Leemos el archivo y definimos los predictores\n",
    "\n",
    "titanic = pd.read_csv('data/titanic_data.csv')\n",
    "\n",
    "for columna in ['Name', 'Ticket', 'Cabin']:\n",
    "    del titanic[columna]\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las listas de predictores por tipo\n",
    "predictores_num = titanic.select_dtypes(['number']).columns\n",
    "predictores_num = predictores_num.delete(-1)                                 # Este será nuestro target\n",
    "predictores_cat = titanic.select_dtypes(['object']).columns\n",
    "\n",
    "print(predictores_num)\n",
    "print(predictores_cat)\n",
    "\n",
    "# Definimos los vectores de predictores y la respuesta\n",
    "y = titanic.Survived\n",
    "#y = np.ravel(y)\n",
    "X = titanic[list(predictores_num) + list(predictores_cat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el proceso análogo al de antes\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan, strategy='median') ),\n",
    "                                      ('scaler',StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocesado = ColumnTransformer(transformers=[('num', numeric_transformer, predictores_num),\n",
    "                                 ('cat', categorical_transformer, predictores_cat)])\n",
    "\n",
    "steps=[('preprocessor', preprocesado),\n",
    "       ('predictor', linear_model.LinearRegression())]\n",
    "\n",
    "pipe = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Estimamos el error del modelo\n",
    "score = cross_val_score(pipe, X, y, cv=10)\n",
    "print(score)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio:\n",
    "\n",
    "Con los datos de 'data/spam.csv' crear un algoritmo de clasificación que detecte si un mail es spam o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leer los datos\n",
    "\n",
    "## Explorar los datos\n",
    "\n",
    "## Definir las columnas numéricas y las categóricas\n",
    "\n",
    "## Definir los predictores y la respuesta\n",
    "\n",
    "### Operaciones a realizar a las variables categóricas\n",
    "\n",
    "### Concatenarlas con las operaciones a realizar a las variables numéricas\n",
    "\n",
    "## Definimos la cañería, o bien realizamos el preprocesado a mano (¡CON MUCHO CUIDADO E INDICÁNDOLO TODO!)\n",
    "\n",
    "## Estimamos el error del modelo por validación cruzada\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
